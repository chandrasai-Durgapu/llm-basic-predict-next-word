{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBxeUFJGNE86rZyChzu5bR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandrasai-Durgapu/llm-basic-predict-next-word/blob/main/llm_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM uses a deep learning architecture that learns word relationships through self-attention. The goal of our language model will be to predict the next word.\n",
        "\n",
        "Here are the six main components weâ€™ll cover:\n",
        "\n",
        "Tokenization,\n",
        "Embedding Layer,\n",
        "Positional Encoding,\n",
        "Self-Attention,\n",
        "Transformer Block,\n",
        "Full Language Model.\n"
      ],
      "metadata": {
        "id": "Jdz3S7zxaghe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "XCRNfc1bP_bY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71c836d3"
      },
      "source": [
        "class SimpleLLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_heads):\n",
        "        super(SimpleLLM, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dim)\n",
        "        self.transformer_blocks = nn.Sequential(*[TransformerBlock(embedding_dim, hidden_dim, num_heads) for _ in range(num_layers)])\n",
        "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.transpose(0, 1) # Transpose for positional encoding\n",
        "        x = self.positional_encoding(x)\n",
        "        x = x.transpose(0, 1) # Transpose back\n",
        "        x = self.transformer_blocks(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b3768e7"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_heads):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embedding_dim, num_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embedding_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attended, _ = self.attention(x)  # SelfAttention returns attention_values and attention_weights\n",
        "        x = self.norm1(x + attended)\n",
        "        forwarded = self.feed_forward(x)\n",
        "        x = self.norm2(x + forwarded)\n",
        "        return x"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e391c25"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self,embedding_dim,num_heads):\n",
        "    super(SelfAttention,self).__init__()\n",
        "    self.Query=nn.Linear(embedding_dim,embedding_dim)\n",
        "    self.Key=nn.Linear(embedding_dim,embedding_dim)\n",
        "    self.Value=nn.Linear(embedding_dim,embedding_dim)\n",
        "  def forward(self,x):\n",
        "    queries= self.Query(x)\n",
        "    keys=self.Key(x)\n",
        "    values=self.Value(x)\n",
        "    scores=torch.bmm(queries,keys.transpose(1,2))/torch.sqrt(torch.tensor(x.size(-1),dtype=torch.float32))\n",
        "    atttention_weights=torch.softmax(scores,dim=-1)\n",
        "    attention_values=torch.bmm(attention_weights,values)\n",
        "    return attention_values,attention_weights"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e087b69e"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_sql_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_sql_len, embedding_dim)\n",
        "        position = torch.arange(0, max_sql_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(1) # Shape: [max_sql_len, 1, embedding_dim]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [sequence_length, batch_size, embedding_dim]\n",
        "        # pe shape: [max_sql_len, 1, embedding_dim]\n",
        "        # We need to add pe to x, broadcasting over the batch dimension (dim 1)\n",
        "        # Slice pe to match the sequence length of x\n",
        "        return x + self.pe[:x.size(0), :, :] # Add pe, broadcasting over batch dimension and slicing sequence length"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dedfef7b"
      },
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embeddings(x)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text,vocab):\n",
        "  return list(vocab.get(word, vocab[\"<UNK>\"]) for word in text.split())"
      ],
      "metadata": {
        "id": "okZgV_HChj4s"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe237106"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_heads):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = SelfAttention(embedding_dim, num_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embedding_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attended, _ = self.attention(x)  # SelfAttention returns attention_values and attention_weights\n",
        "        x = self.norm1(x + attended)\n",
        "        forwarded = self.feed_forward(x)\n",
        "        x = self.norm2(x + forwarded)\n",
        "        return x"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "785e196d"
      },
      "source": [
        "class SimpleLLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_heads):\n",
        "        super(SimpleLLM, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_encoding = PositionalEncoding(embedding_dim)\n",
        "        self.transformer_blocks = nn.Sequential(*[TransformerBlock(embedding_dim, hidden_dim, num_heads) for _ in range(num_layers)])\n",
        "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.transpose(0, 1) # Transpose for positional encoding\n",
        "        x = self.positional_encoding(x)\n",
        "        x = x.transpose(0, 1) # Transpose back\n",
        "        x = self.transformer_blocks(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aec7af3"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self,embedding_dim,num_heads):\n",
        "    super(SelfAttention,self).__init__()\n",
        "    self.Query=nn.Linear(embedding_dim,embedding_dim)\n",
        "    self.Key=nn.Linear(embedding_dim,embedding_dim)\n",
        "    self.Value=nn.Linear(embedding_dim,embedding_dim)\n",
        "  def forward(self,x):\n",
        "    queries= self.Query(x)\n",
        "    keys=self.Key(x)\n",
        "    values=self.Value(x)\n",
        "    scores=torch.bmm(queries,keys.transpose(1,2))/torch.sqrt(torch.tensor(x.size(-1),dtype=torch.float32))\n",
        "    attention_weights=torch.softmax(scores,dim=-1)\n",
        "    attention_values=torch.bmm(attention_weights,values)\n",
        "    return attention_values,attention_weights"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc2c9b97",
        "outputId": "b9d0c442-8ac3-44dd-a178-fb3794a8b20e"
      },
      "source": [
        "vocab = {\n",
        "    \"hello\": 0, \"world\": 1, \"how\": 2, \"are\": 3, \"you\": 4,\n",
        "    \"good\": 5, \"morning\": 6, \"evening\": 7, \"night\": 8,\n",
        "    \"friend\": 9, \"nice\": 10, \"to\": 11, \"meet\": 12, \"learning\": 13,\n",
        "    \"AI\": 14, \"is\": 15, \"fun\": 16, \"great\": 17, \"awesome\": 18,\n",
        "    \"day\": 19, \"doing\": 20, \"today\": 21, \"hope\": 22, \"all\": 23,\n",
        "    \"well\": 24, \"<UNK>\": 25 # Remove empty string and adjust indices, <UNK> is now 25\n",
        "}\n",
        "\n",
        "vocab_size = len(vocab) # vocab_size will now be 26\n",
        "embedding_dim = 16\n",
        "hidden_dim = 32\n",
        "num_layers = 2\n",
        "num_heads = 4 # Add num_heads\n",
        "\n",
        "model = SimpleLLM(vocab_size, embedding_dim, hidden_dim, num_layers, num_heads) # Pass num_heads\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "data = [\n",
        "    \"hello world how are you\",\n",
        "    \"how are you hello world\",\n",
        "    \"good morning friend\",\n",
        "    \"nice to meet you\",\n",
        "    \"learning AI is fun\",\n",
        "    \"have a great day\",\n",
        "    \"hope you are doing well\",\n",
        "    \"AI is awesome\",\n",
        "    \"what are you doing today\",\n",
        "    \"good evening to all\"\n",
        "]\n",
        "\n",
        "tokenized_data = [tokenize(sentence, vocab) for sentence in data]\n",
        "\n",
        "for epoch in range(100):\n",
        "    for sentence in tokenized_data:\n",
        "        for i in range(1, len(sentence)):\n",
        "            input_seq = torch.tensor(sentence[:i]).unsqueeze(0)\n",
        "            target = torch.tensor([sentence[i]], dtype=torch.long) # Ensure target is shape [1]\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_seq)\n",
        "            loss = criterion(output[:, -1, :], target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 4.461989402770996\n",
            "Epoch 10, Loss: 1.92235267162323\n",
            "Epoch 20, Loss: 0.6763872504234314\n",
            "Epoch 30, Loss: 0.24692803621292114\n",
            "Epoch 40, Loss: 0.12351427227258682\n",
            "Epoch 50, Loss: 0.06678084284067154\n",
            "Epoch 60, Loss: 0.041416414082050323\n",
            "Epoch 70, Loss: 0.02816096507012844\n",
            "Epoch 80, Loss: 0.020215198397636414\n",
            "Epoch 90, Loss: 0.015071799978613853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d16cf96",
        "outputId": "904e7515-ce5a-490b-dcf1-1da1b439d71c"
      },
      "source": [
        "def predict_next_word(model, tokenizer, vocab, input_text, top_k=1):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Tokenize the input text\n",
        "        tokenized_input = tokenizer(input_text, vocab)\n",
        "        # Convert tokens to tensor and add batch dimension\n",
        "        input_tensor = torch.tensor(tokenized_input).unsqueeze(0)\n",
        "        # Get model output (logits)\n",
        "        output = model(input_tensor)\n",
        "        # Get the logits for the last token in the sequence\n",
        "        last_token_logits = output[:, -1, :]\n",
        "        # Get the probabilities\n",
        "        probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "        # Get the top k predicted token indices\n",
        "        top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n",
        "        # Convert indices back to words\n",
        "        idx_to_word = {idx: word for word, idx in vocab.items()}\n",
        "        predicted_words = [idx_to_word[index.item()] for index in top_k_indices[0]]\n",
        "        return predicted_words\n",
        "\n",
        "# Example usage:\n",
        "input_sentence = \"hello world how are\"\n",
        "predicted_words = predict_next_word(model, tokenize, vocab, input_sentence, top_k=3)\n",
        "print(f\"Input: {input_sentence}\")\n",
        "print(f\"Predicted next words: {predicted_words}\")\n",
        "\n",
        "input_sentence = \"good morning\"\n",
        "predicted_words = predict_next_word(model, tokenize, vocab, input_sentence, top_k=3)\n",
        "print(f\"Input: {input_sentence}\")\n",
        "print(f\"Predicted next words: {predicted_words}\")"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: hello world how are\n",
            "Predicted next words: ['you', 'doing', 'to']\n",
            "Input: good morning\n",
            "Predicted next words: ['friend', 'doing', 'all']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34a89fe6"
      },
      "source": [],
      "execution_count": 166,
      "outputs": []
    }
  ]
}